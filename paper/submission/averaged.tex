\section{Approximation by Averaged
Systems}\label{sec:averaged-system}

Consider the averaged system $\Sigma_a$ of $\Sigma_b$ for each time interval
$k$:
\[ (\Sigma_a) \hspace{2em} \dot{\overbar{x}} (t) = A \overbar{x} (t) + B
   \eta_k + Ew (t), \hspace{2em} t \in [kT, (k + 1) T) \]
where $\eta_k = \frac{1}{T} \int_{kT}^{(k + 1) T} u (t) \mathd t$ is the
{\emph{utilization}} of $u (t)$ during the interval $[kT, (k + 1) T]$,
$\eta_k$ is a vector of real numbers between $0$ and $1$: $\eta_k \in [0,
1]^m$. At each instant $kT$, the state $\overbar{x} (kT)$ is reset to the
real state $x (kT)$. The deviation $e (t) = x (t) - \overbar{x} (t)$ follows
the dynamics
\begin{equation}
  \dot{e} (t) = A (x (t) - \overbar{x} (t)) + B (u (t) - \eta_k) = Ae (t) + B
  (u (t) - \eta_k) \label{eq:err-dynamics}
\end{equation}
for all $t \in [kT, (k + 1) T)$ and $e (kT) = 0$. At the next instant $(k + 1)
T$, the error is given by the solution of the differential equation~\eqref{eq:err-dynamics}
\[ e ((k + 1) T) = \int_{kT}^{(k + 1) T} \mathe^{A ((k + 1) T - s)} B (u (s) -
   \eta_k) \mathd s \]
A tight bound on $e ((k + 1) T)$ can be obtained, which is dependent on the
value of $\eta_k$. In this section, we make a practical assumption that the
state matrix $A$ is diagonalizable with real eigenvalues, that is $VA = DV$
where $D$ is a diagonal matrix with the eigenvalues of $A$ and $V$ is a matrix
whose columns are the corresponding right eigenvectors. In many practical
systems, especially in our targeted energy applications, this assumption is
satisfied. We can then transform the state with $V$ by defining a new state $z
= Vx$, whose dynamics are
\[ \dot{z} = V \dot{x} = VAx + VBu + VEw = Dz + VBu + VEw \]
and which is subject to state constraint $z (t) \in V\mathcal{X}$. This new
model is in the form of $(\Sigma_b)$ with a diagonal state matrix. As a
consequence, we can assume without loss of generality that the original system
$\Sigma_b$ has diagonal state matrix $A$ because otherwise, we can always
equivalently transform $\Sigma_b$ to achieve this. Let $a_i$ be the
$i^{\text{th}}$ diagonal element of $A$, for $i = 1, \ldots, n$.

{\noindent}\textbf{Note . }We can still use our approach when $A$ is not
diagonalizable, however the error bounds obtained in
\cref{thm:averaged-sys-bound} must be generalized. For example, we can
use the Jordan normal form to transform $A$ into a block diagonal matrix, and
obtain bounds in a way similar to that in
\cref{thm:averaged-sys-bound}. The made assumption simplifies the
discussion of our results. It does not limit the practicality of our approach
much because, as we mentioned, many practical applications satisfy the
assumption.{\hspace*{\fill}}{\medskip}

\begin{theorem}
  \label{thm:averaged-sys-bound}Consider each element $e_i ((k + 1) T)$ of the
  error $e ((k + 1) T)$, $1 \leqslant i \leqslant n$. If $a_i = 0$ then $e_i
  ((k + 1) T) = 0$. Otherwise, $e_i ((k + 1) T)$ is bounded by
  \[ \xi_i  (\underline{\varepsilon}_i - b_i \eta_k) = \underline{e}_i
     (\eta_k) \leqslant e_i ((k + 1) T) \leqslant \overbar{e}_i (\eta_k) =
     \xi_i  (\overbar{\varepsilon}_i - b_i \eta_k) \]
  for all $u (t)$ such that $\eta_k = \frac{1}{T} \int_{kT}^{(k + 1) T} u (t)
  \mathd t$, where $b_i$ is the $i^{\text{th}}$ row of $B$,
  $\overbar{\varepsilon}_i$ is the sum of all positive elements of $b_i$,
  $\underline{\varepsilon}_i$ is the sum of all negative elements of $b_i$,
  and
  \[ \xi_i = \left\{ \begin{array}{ll}
       \frac{1}{a_i} (\mathe^{a_i T} - 1) - T & \text{if } a_i > 0\\
       \frac{1}{a_i} (\mathe^{a_i T} - 1) - T \mathe^{a_i T} & \text{if } a_i
       < 0
     \end{array} \right._{} \]
\end{theorem}

\begin{proof}
  Because $A$ is diagonal, each element $e_i ((k + 1) T)$ can be calculated
  independently
  \[ e_i ((k + 1) T) = \int_{kT}^{(k + 1) T} \mathe^{a_i ((k + 1) T - s)} b_i 
     (u (s) - \eta_k) \mathd s \text{.} \]
  If $a_i = 0$ then
  \[ e_i ((k + 1) T) = \int_{kT}^{(k + 1) T} b_i  (u (s) - \eta_k) \mathd s =
     b_i  \left( \int_{kT}^{(k + 1) T} u (s) \mathd s - T \eta_k \right) = 0
  \]
  where the last equality comes from the definition of $\eta_k$. Consider the
  case $a_i \neq 0$. Observe that $\int_{kT}^{(k + 1) T} \lambda_i b_i  (u (s)
  - \eta_k) \mathd s = 0$ for any constant $\lambda_i$. Consequently
  \[ e_i ((k + 1) T) = \int_{kT}^{(k + 1) T} (\mathe^{a_i ((k + 1) T - s)} -
     \lambda_i ) b_i  (u (s) - \eta_k) \mathd s \]
  where $\lambda_i = 1$ if $a_i > 0$ and $\lambda_i = \mathe^{a_i T}$ if $a_i
  < 0$. It is straightforward to show that $\mathe^{a_i ((k + 1) T - s)} -
  \lambda_i \geqslant 0$ for all $s \in [kT, (k + 1) T]$. Because $u (s) \in
  \{ 0, 1 \}^m$ for all $s$, $\underline{\varepsilon}_i \leqslant b_i u (s)
  \leqslant \overbar{\varepsilon}_i$ with $\underline{\varepsilon}_i$ and
  $\overbar{\varepsilon}_i$ defined as above. Therefore $(\mathe^{a_i ((k +
  1) T - s)} - \lambda_i )  (\underline{\varepsilon}_i - b_i \eta_k) \leqslant
  (\mathe^{a_i ((k + 1) T - s)} - \lambda_i ) b_i  (u (s) - \eta_k) \leqslant
  (\mathe^{a_i ((k + 1) T - s)} - \lambda_i )  (\overbar{\varepsilon}_i - b_i
  \eta_k)$. It follows that
  \[ \xi_i  (\underline{\varepsilon}_i - b_i \eta_k) \leqslant \int_{kT}^{(k +
     1) T} (\mathe^{a_i ((k + 1) T - s)} - \lambda_i ) b_i  (u (s) - \eta_k)
     \mathd s \leqslant \xi_i  (\overbar{\varepsilon}_i - b_i \eta_k) \]
  where $\xi_i = \int_{kT}^{(k + 1) T} (\mathe^{a_i ((k + 1) T - s)} -
  \lambda_i ) \mathd s = \frac{1}{a_i} (\mathe^{a_i T} - 1) - \lambda_i T$.
  The theorem is proved.
\end{proof}

The bound range $[\underline{e}_i (\eta_k), \overbar{e}_i (\eta_k)]$ given by
\cref{thm:averaged-sys-bound} expands as $T$
increases. Therefore, as $k$ increases, $\overbar{x} (kT)$ may
deviate further and further from $x (kT)$, which is undesirable when the state
constraint needs to be maintained. For this reason, to keep the deviation
within a tight bound, we reset the state $\overbar{x}$ of $\Sigma_a$ to the
measured state $x$ of $\Sigma_b$ at each $kT$, so that $e (kT)$ is reset to
$0$. Therefore, the difference between $x ((k + 1) T)$ and the next value of
$\overbar{x} (k)$ under $\eta_k$ is always bounded in $[\underline{e}_i
(\eta_k), \overbar{e}_i (\eta_k)]$ for all $k$.
Note that the bound values $\underline{e}_i(\eta_{k})$ and $\overbar{e}_i(\eta_{k})$ are linear in $\eta_{k}$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "emsoft15gs"
%%% End:
